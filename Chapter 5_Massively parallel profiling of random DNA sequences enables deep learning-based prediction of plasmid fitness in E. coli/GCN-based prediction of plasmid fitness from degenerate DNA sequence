"""
Train a 1D CNN encoder–decoder model to predict plasmid fitness (log2 fold-change)
from one-hot encoded DNA sequences.

This script reproduces the architecture and training logic used in the thesis,
with paths and dataset loading abstracted for reproducibility.
"""

import os
import random
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from datetime import datetime

# Reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Model components
class Encoder(nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.conv1 = nn.Conv1d(in_ch, out_ch, kernel_size=3, padding=1)
        self.bn1   = nn.BatchNorm1d(out_ch)
        self.conv2 = nn.Conv1d(out_ch, out_ch, kernel_size=5, padding=2)
        self.bn2   = nn.BatchNorm1d(out_ch)
        self.conv3 = nn.Conv1d(out_ch, out_ch, kernel_size=7, stride=2, padding=3)

    def forward(self, x, condense=True):
        x = F.relu(self.conv1(x))
        residual = x
        x = self.bn1(x)
        x = F.relu(self.conv2(x))
        x = self.bn2(x)
        x = x + residual
        if condense:
            x = F.relu(self.conv3(x))
        return x, residual


class Decoder(nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.up = nn.ConvTranspose1d(in_ch, out_ch, kernel_size=2, stride=2)
        self.conv1 = nn.Conv1d(in_ch + out_ch, out_ch, kernel_size=5, padding=2)
        self.bn1 = nn.BatchNorm1d(out_ch)
        self.conv2 = nn.Conv1d(out_ch, out_ch, kernel_size=5, padding=2)
        self.bn2 = nn.BatchNorm1d(out_ch)

    def forward(self, x, skip):
        x = F.relu(self.up(x))
        x = torch.cat([x, skip], dim=1)
        x = F.relu(self.conv1(x))
        x = self.bn1(x)
        x = F.relu(self.conv2(x))
        x = self.bn2(x)
        return x

class FitnessCNN(nn.Module):
    """
    Encoder–decoder CNN followed by fully connected regression head
    """
    def __init__(self, hidden_ch=128):
        super().__init__()

        self.enc1 = Encoder(4, hidden_ch)
        self.enc2 = Encoder(hidden_ch, hidden_ch)
        self.enc3 = Encoder(hidden_ch, hidden_ch)
        self.enc4 = Encoder(hidden_ch, hidden_ch)

        self.dec1 = Decoder(hidden_ch, hidden_ch)
        self.dec2 = Decoder(hidden_ch, hidden_ch)
        self.dec3 = Decoder(hidden_ch, hidden_ch)

        self.fc = nn.Sequential(
            nn.Linear(192 * hidden_ch, hidden_ch),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_ch, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )

    def forward(self, x):
        # x: (B, 192, 4)
        x = x.permute(0, 2, 1)

        x, s1 = self.enc1(x)
        x, s2 = self.enc2(x)
        x, s3 = self.enc3(x)
        x, _  = self.enc4(x, condense=False)

        x = self.dec1(x, s3)
        x = self.dec2(x, s2)
        x = self.dec3(x, s1)

        x = x.permute(0, 2, 1)
        x = torch.flatten(x, start_dim=1)
        return self.fc(x)

# Dataset stub
class SequenceDataset(torch.utils.data.Dataset):
    """
    Expects:
      sequences: (N, 192, 4) float32
      targets:   (N,) float32
    """
    def __init__(self, sequences, targets):
        self.x = torch.tensor(sequences, dtype=torch.float32)
        self.y = torch.tensor(targets, dtype=torch.float32)

    def __len__(self):
        return len(self.y)

    def __getitem__(self, idx):
        return self.x[idx], self.y[idx]

# Training loop
def train(model, train_loader, val_loader, epochs=50, lr=1e-3):
    model.to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=3e-4)
    criterion = nn.HuberLoss()

    best_val = float("inf")

    for epoch in range(epochs):
        model.train()
        train_loss = 0.0

        for x, y in train_loader:
            x, y = x.to(device), y.to(device).unsqueeze(1)
            optimizer.zero_grad()
            preds = model(x)
            loss = criterion(preds, y)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()

        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for x, y in val_loader:
                x, y = x.to(device), y.to(device).unsqueeze(1)
                preds = model(x)
                val_loss += criterion(preds, y).item()

        train_loss /= len(train_loader)
        val_loss   /= len(val_loader)

        print(f"Epoch {epoch+1:03d} | Train {train_loss:.4f} | Val {val_loss:.4f}")

        if val_loss < best_val:
            best_val = val_loss
            torch.save(model.state_dict(), "best_model.pt")

    print("Training complete.")


if __name__ == "__main__":
    """
    Replace this block with real data loading in reproducibility/
    """
    raise RuntimeError(
        "This script defines the full training pipeline.\n"
        "Please load your dataset and call train() explicitly."
    )
